{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Xception_35_bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from os.path import isfile, isdir, join, pardir\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Dense, Flatten, GlobalAveragePooling2D, Dropout\n",
    "from keras import optimizers\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "from keras.utils import to_categorical\n",
    "from keras import regularizers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.applications import xception\n",
    "from keras.applications.xception import Xception\n",
    "\n",
    "import pydot\n",
    "import cv2\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将训练集数据分为5份"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_group = np.array([['p002', 'p012', 'p014', 'p015', 'p016'], \n",
    "                ['p021', 'p022', 'p024', 'p026', 'p035'],\n",
    "                ['p039', 'p041', 'p042', 'p045', 'p047'],\n",
    "                ['p049', 'p050', 'p051', 'p052', 'p056'],\n",
    "                ['p061', 'p064', 'p066', 'p072', 'p075', 'p081']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "driver_dataset_folder_path = join(pardir, 'data', 'driver_dataset')\n",
    "driver_imgs_list = pd.read_csv(join(pardir, 'data', 'driver_imgs_list.csv'))\n",
    "image_valid_folder_path = join(pardir, 'data', 'driver_dataset', 'valid')\n",
    "\n",
    "test_image_path = join(driver_dataset_folder_path, 'test')\n",
    "xception_test_datagen = ImageDataGenerator(preprocessing_function=xception.preprocess_input)\n",
    "xception_test_generator = xception_test_datagen.flow_from_directory(\n",
    "    test_image_path,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    class_mode='categorical')\n",
    "\n",
    "image_names = []\n",
    "for root, dirs, file_name in os.walk(join(test_image_path, '0')):\n",
    "    image_names.append(file_name)\n",
    "image_names = np.array(image_names).transpose()\n",
    "\n",
    "def rmrf_mkdir(dirname):\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_dir = join(pardir, 'data', 'driver_dataset', 'train1')\n",
    "valid1_dir = join(pardir, 'data', 'driver_dataset', 'valid1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmrf_mkdir(train1_dir)\n",
    "rmrf_mkdir(valid1_dir)\n",
    "\n",
    "train_driver_id1 = np.array([driver_group[0],\n",
    "                           driver_group[1],\n",
    "                           driver_group[2],\n",
    "                           driver_group[3]]).flatten()\n",
    "valid_driver_id1 = np.array(driver_group[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p002', 'p012', 'p014', 'p015', 'p016', 'p021', 'p022', 'p024',\n",
       "       'p026', 'p035', 'p039', 'p041', 'p042', 'p045', 'p047', 'p049',\n",
       "       'p050', 'p051', 'p052', 'p056'], dtype='<U4')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_driver_id1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in valid_driver_id1:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(valid1_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(valid1_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(valid1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in train_driver_id1:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(train1_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(train1_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(train1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xception_train_datagen = ImageDataGenerator(\n",
    "    rotation_range=10.,\n",
    "    width_shift_range=0.05,\n",
    "    height_shift_range=0.05,\n",
    "    preprocessing_function=xception.preprocess_input)\n",
    "\n",
    "xception_valid_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=xception.preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17778 images belonging to 10 classes.\n",
      "Found 4646 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "xception_train_generator1 = xception_train_datagen.flow_from_directory(\n",
    "    train1_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')\n",
    "\n",
    "xception_valid_generator1 = xception_valid_datagen.flow_from_directory(\n",
    "    valid1_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 block1_conv1\n",
      "2 block1_conv1_bn\n",
      "3 block1_conv1_act\n",
      "4 block1_conv2\n",
      "5 block1_conv2_bn\n",
      "6 block1_conv2_act\n",
      "7 block2_sepconv1\n",
      "8 block2_sepconv1_bn\n",
      "9 block2_sepconv2_act\n",
      "10 block2_sepconv2\n",
      "11 block2_sepconv2_bn\n",
      "12 conv2d_5\n",
      "13 block2_pool\n",
      "14 batch_normalization_5\n",
      "15 add_13\n",
      "16 block3_sepconv1_act\n",
      "17 block3_sepconv1\n",
      "18 block3_sepconv1_bn\n",
      "19 block3_sepconv2_act\n",
      "20 block3_sepconv2\n",
      "21 block3_sepconv2_bn\n",
      "22 conv2d_6\n",
      "23 block3_pool\n",
      "24 batch_normalization_6\n",
      "25 add_14\n",
      "26 block4_sepconv1_act\n",
      "27 block4_sepconv1\n",
      "28 block4_sepconv1_bn\n",
      "29 block4_sepconv2_act\n",
      "30 block4_sepconv2\n",
      "31 block4_sepconv2_bn\n",
      "32 conv2d_7\n",
      "33 block4_pool\n",
      "34 batch_normalization_7\n",
      "35 add_15\n",
      "36 block5_sepconv1_act\n",
      "37 block5_sepconv1\n",
      "38 block5_sepconv1_bn\n",
      "39 block5_sepconv2_act\n",
      "40 block5_sepconv2\n",
      "41 block5_sepconv2_bn\n",
      "42 block5_sepconv3_act\n",
      "43 block5_sepconv3\n",
      "44 block5_sepconv3_bn\n",
      "45 add_16\n",
      "46 block6_sepconv1_act\n",
      "47 block6_sepconv1\n",
      "48 block6_sepconv1_bn\n",
      "49 block6_sepconv2_act\n",
      "50 block6_sepconv2\n",
      "51 block6_sepconv2_bn\n",
      "52 block6_sepconv3_act\n",
      "53 block6_sepconv3\n",
      "54 block6_sepconv3_bn\n",
      "55 add_17\n",
      "56 block7_sepconv1_act\n",
      "57 block7_sepconv1\n",
      "58 block7_sepconv1_bn\n",
      "59 block7_sepconv2_act\n",
      "60 block7_sepconv2\n",
      "61 block7_sepconv2_bn\n",
      "62 block7_sepconv3_act\n",
      "63 block7_sepconv3\n",
      "64 block7_sepconv3_bn\n",
      "65 add_18\n",
      "66 block8_sepconv1_act\n",
      "67 block8_sepconv1\n",
      "68 block8_sepconv1_bn\n",
      "69 block8_sepconv2_act\n",
      "70 block8_sepconv2\n",
      "71 block8_sepconv2_bn\n",
      "72 block8_sepconv3_act\n",
      "73 block8_sepconv3\n",
      "74 block8_sepconv3_bn\n",
      "75 add_19\n",
      "76 block9_sepconv1_act\n",
      "77 block9_sepconv1\n",
      "78 block9_sepconv1_bn\n",
      "79 block9_sepconv2_act\n",
      "80 block9_sepconv2\n",
      "81 block9_sepconv2_bn\n",
      "82 block9_sepconv3_act\n",
      "83 block9_sepconv3\n",
      "84 block9_sepconv3_bn\n",
      "85 add_20\n",
      "86 block10_sepconv1_act\n",
      "87 block10_sepconv1\n",
      "88 block10_sepconv1_bn\n",
      "89 block10_sepconv2_act\n",
      "90 block10_sepconv2\n",
      "91 block10_sepconv2_bn\n",
      "92 block10_sepconv3_act\n",
      "93 block10_sepconv3\n",
      "94 block10_sepconv3_bn\n",
      "95 add_21\n",
      "96 block11_sepconv1_act\n",
      "97 block11_sepconv1\n",
      "98 block11_sepconv1_bn\n",
      "99 block11_sepconv2_act\n",
      "100 block11_sepconv2\n",
      "101 block11_sepconv2_bn\n",
      "102 block11_sepconv3_act\n",
      "103 block11_sepconv3\n",
      "104 block11_sepconv3_bn\n",
      "105 add_22\n",
      "106 block12_sepconv1_act\n",
      "107 block12_sepconv1\n",
      "108 block12_sepconv1_bn\n",
      "109 block12_sepconv2_act\n",
      "110 block12_sepconv2\n",
      "111 block12_sepconv2_bn\n",
      "112 block12_sepconv3_act\n",
      "113 block12_sepconv3\n",
      "114 block12_sepconv3_bn\n",
      "115 add_23\n",
      "116 block13_sepconv1_act\n",
      "117 block13_sepconv1\n",
      "118 block13_sepconv1_bn\n",
      "119 block13_sepconv2_act\n",
      "120 block13_sepconv2\n",
      "121 block13_sepconv2_bn\n",
      "122 conv2d_8\n",
      "123 block13_pool\n",
      "124 batch_normalization_8\n",
      "125 add_24\n",
      "126 block14_sepconv1\n",
      "127 block14_sepconv1_bn\n",
      "128 block14_sepconv1_act\n",
      "129 block14_sepconv2\n",
      "130 block14_sepconv2_bn\n",
      "131 block14_sepconv2_act\n"
     ]
    }
   ],
   "source": [
    "model_xception = Xception(include_top=False, weights='imagenet')\n",
    "\n",
    "for i, layer in enumerate(model_xception.layers):\n",
    "    print (i, layer.name)\n",
    "    \n",
    "for i in range(35):\n",
    "    model_xception.layers[i].trainable = False\n",
    "        \n",
    "input = Input(shape=(299, 299, 3),name = 'image_input')\n",
    " \n",
    "output_xception_conv = model_xception(input)\n",
    "   \n",
    "x = GlobalAveragePooling2D()(output_xception_conv)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "adam = optimizers.Adam(lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model(inputs=input, outputs=x) \n",
    "\n",
    "model_1.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1112/1112 [==============================] - 609s 547ms/step - loss: 0.2157 - categorical_accuracy: 0.9402 - val_loss: 1.0442 - val_categorical_accuracy: 0.7318\n",
      "Epoch 2/2\n",
      "1112/1112 [==============================] - 599s 539ms/step - loss: 0.0262 - categorical_accuracy: 0.9929 - val_loss: 0.7809 - val_categorical_accuracy: 0.8052\n"
     ]
    }
   ],
   "source": [
    "history_1 = model_1.fit_generator(\n",
    "    xception_train_generator1,\n",
    "    epochs=2,\n",
    "    validation_data=xception_valid_generator1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xception model saved.\n"
     ]
    }
   ],
   "source": [
    "model_1.save(join(pardir, 'model', 'xception_1.h5'))\n",
    "print(\"xception model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model_xception_1 = load_model(join(pardir, 'model', 'xception_1.h5'))\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2492/2492 [==============================] - 1285s 516ms/step\n",
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_xception_1 = model_xception_1.predict_generator(xception_test_generator, verbose=1)\n",
    "print(pred_xception_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xception_1 = np.append(image_names, pred_xception_1, axis = 1)\n",
    "predict_result_xception_1 = pd.DataFrame(result_xception_1, \n",
    "            columns=['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "predict_result_xception_1.to_csv('predict_result_xception_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_dir = join(pardir, 'data', 'driver_dataset', 'train2')\n",
    "valid2_dir = join(pardir, 'data', 'driver_dataset', 'valid2')\n",
    "\n",
    "rmrf_mkdir(train2_dir)\n",
    "rmrf_mkdir(valid2_dir)\n",
    "\n",
    "train_driver_id2 = np.concatenate((driver_group[0],\n",
    "                                   driver_group[2],\n",
    "                                   driver_group[3]))\n",
    "train_driver_id2 = np.concatenate((train_driver_id2,\n",
    "                                   driver_group[4]))\n",
    "valid_driver_id2 = np.array(driver_group[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p002', 'p012', 'p014', 'p015', 'p016', 'p039', 'p041', 'p042',\n",
       "       'p045', 'p047', 'p049', 'p050', 'p051', 'p052', 'p056', 'p061',\n",
       "       'p064', 'p066', 'p072', 'p075', 'p081'], dtype='<U4')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_driver_id2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in valid_driver_id2:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(valid2_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(valid2_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(valid2_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in train_driver_id2:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(train2_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(train2_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(train2_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16684 images belonging to 10 classes.\n",
      "Found 5740 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "xception_train_generator2 = xception_train_datagen.flow_from_directory(\n",
    "    train2_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')\n",
    "\n",
    "xception_valid_generator2 = xception_valid_datagen.flow_from_directory(\n",
    "    valid2_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model(inputs=input, outputs=x) \n",
    "\n",
    "model_2.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1043/1043 [==============================] - 592s 567ms/step - loss: 0.0667 - categorical_accuracy: 0.9820 - val_loss: 0.1312 - val_categorical_accuracy: 0.9646\n",
      "Epoch 2/2\n",
      "1043/1043 [==============================] - 585s 561ms/step - loss: 0.0149 - categorical_accuracy: 0.9957 - val_loss: 0.0779 - val_categorical_accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "history_2 = model_2.fit_generator(\n",
    "    xception_train_generator2,\n",
    "    epochs=2,\n",
    "    validation_data=xception_valid_generator2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xception model saved.\n"
     ]
    }
   ],
   "source": [
    "model_2.save(join(pardir, 'model', 'xception_2.h5'))\n",
    "print(\"xception model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model_xception_2 = load_model(join(pardir, 'model', 'xception_2.h5'))\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2492/2492 [==============================] - 1302s 522ms/step\n",
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_xception_2 = model_xception_2.predict_generator(xception_test_generator, verbose=1)\n",
    "print(pred_xception_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xception_2 = np.append(image_names, pred_xception_2, axis = 1)\n",
    "predict_result_xception_2 = pd.DataFrame(result_xception_2, \n",
    "            columns=['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "predict_result_xception_2.to_csv('predict_result_xception_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train3_dir = join(pardir, 'data', 'driver_dataset', 'train3')\n",
    "valid3_dir = join(pardir, 'data', 'driver_dataset', 'valid3')\n",
    "\n",
    "rmrf_mkdir(train3_dir)\n",
    "rmrf_mkdir(valid3_dir)\n",
    "\n",
    "train_driver_id3 = np.concatenate((driver_group[0],\n",
    "                                   driver_group[1],\n",
    "                                   driver_group[3]))\n",
    "train_driver_id3 = np.concatenate((train_driver_id3,\n",
    "                                   driver_group[4]))\n",
    "valid_driver_id3 = np.array(driver_group[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in valid_driver_id3:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(valid3_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(valid3_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(valid3_dir, 'c%s'%i,row['img']))\n",
    "                #os.symlink(join(image_path, row['img']), join(valid1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in train_driver_id3:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(train3_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(train3_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(train3_dir, 'c%s'%i,row['img']))\n",
    "                #os.symlink(join(image_path, row['img']), join(train1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19018 images belonging to 10 classes.\n",
      "Found 3406 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "xception_train_generator3 = xception_train_datagen.flow_from_directory(\n",
    "    train3_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')\n",
    "\n",
    "xception_valid_generator3 = xception_valid_datagen.flow_from_directory(\n",
    "    valid3_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Model(inputs=input, outputs=x) \n",
    "\n",
    "model_3.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1189/1189 [==============================] - 628s 528ms/step - loss: 0.2286 - categorical_accuracy: 0.9344 - val_loss: 0.2303 - val_categorical_accuracy: 0.9331\n",
      "Epoch 2/2\n",
      "1189/1189 [==============================] - 621s 522ms/step - loss: 0.0288 - categorical_accuracy: 0.9928 - val_loss: 0.4101 - val_categorical_accuracy: 0.8902\n"
     ]
    }
   ],
   "source": [
    "history_3 = model_3.fit_generator(\n",
    "    xception_train_generator3,\n",
    "    epochs=2,\n",
    "    validation_data=xception_valid_generator3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xception model saved.\n"
     ]
    }
   ],
   "source": [
    "model_3.save(join(pardir, 'model', 'xception_3.h5'))\n",
    "print(\"xception model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model_xception_3 = load_model(join(pardir, 'model', 'xception_3.h5'))\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2492/2492 [==============================] - 1304s 523ms/step\n",
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_xception_3 = model_xception_3.predict_generator(xception_test_generator, verbose=1)\n",
    "print(pred_xception_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xception_3 = np.append(image_names, pred_xception_3, axis = 1)\n",
    "predict_result_xception_3 = pd.DataFrame(result_xception_3, \n",
    "            columns=['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "predict_result_xception_3.to_csv('predict_result_xception_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train4_dir = join(pardir, 'data', 'driver_dataset', 'train4')\n",
    "valid4_dir = join(pardir, 'data', 'driver_dataset', 'valid4')\n",
    "\n",
    "rmrf_mkdir(train4_dir)\n",
    "rmrf_mkdir(valid4_dir)\n",
    "\n",
    "train_driver_id4 = np.concatenate((driver_group[0],\n",
    "                                   driver_group[1],\n",
    "                                   driver_group[2]))\n",
    "train_driver_id4 = np.concatenate((train_driver_id4,\n",
    "                                   driver_group[4]))\n",
    "valid_driver_id4 = np.array(driver_group[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in valid_driver_id4:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(valid4_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(valid4_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(valid4_dir, 'c%s'%i,row['img']))\n",
    "                #os.symlink(join(image_path, row['img']), join(valid1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in train_driver_id4:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(train4_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(train4_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(train4_dir, 'c%s'%i,row['img']))\n",
    "                #os.symlink(join(image_path, row['img']), join(train1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18169 images belonging to 10 classes.\n",
      "Found 4255 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "xception_train_generator4 = xception_train_datagen.flow_from_directory(\n",
    "    train4_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')\n",
    "\n",
    "xception_valid_generator4 = xception_valid_datagen.flow_from_directory(\n",
    "    valid4_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Model(inputs=input, outputs=x) \n",
    "\n",
    "model_4.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1136/1136 [==============================] - 617s 543ms/step - loss: 0.0518 - categorical_accuracy: 0.9868 - val_loss: 0.0834 - val_categorical_accuracy: 0.9791\n",
      "Epoch 2/2\n",
      "1136/1136 [==============================] - 614s 540ms/step - loss: 0.0123 - categorical_accuracy: 0.9964 - val_loss: 0.1620 - val_categorical_accuracy: 0.9741\n"
     ]
    }
   ],
   "source": [
    "history_4 = model_4.fit_generator(\n",
    "    xception_train_generator4,\n",
    "    epochs=2,\n",
    "    validation_data=xception_valid_generator4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xception model saved.\n"
     ]
    }
   ],
   "source": [
    "model_4.save(join(pardir, 'model', 'xception_4.h5'))\n",
    "print(\"xception model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model_xception_4 = load_model(join(pardir, 'model', 'xception_4.h5'))\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2492/2492 [==============================] - 1291s 518ms/step\n",
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_xception_4 = model_xception_4.predict_generator(xception_test_generator, verbose=1)\n",
    "print(pred_xception_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xception_4 = np.append(image_names, pred_xception_4, axis = 1)\n",
    "predict_result_xception_4 = pd.DataFrame(result_xception_4, \n",
    "            columns=['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "predict_result_xception_4.to_csv('predict_result_xception_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train5_dir = join(pardir, 'data', 'driver_dataset', 'train5')\n",
    "valid5_dir = join(pardir, 'data', 'driver_dataset', 'valid5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmrf_mkdir(train5_dir)\n",
    "rmrf_mkdir(valid5_dir)\n",
    "\n",
    "train_driver_id5 = np.concatenate((driver_group[1],\n",
    "                                   driver_group[2],\n",
    "                                   driver_group[3]))\n",
    "train_driver_id5 = np.concatenate((train_driver_id5,\n",
    "                                   driver_group[4]))\n",
    "valid_driver_id5 = np.array(driver_group[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in valid_driver_id5:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(valid5_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(valid5_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(valid5_dir, 'c%s'%i,row['img']))\n",
    "                #os.symlink(join(image_path, row['img']), join(valid1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in driver_imgs_list.iterrows():\n",
    "    if row['subject'] in train_driver_id5:\n",
    "        for i in range(10):\n",
    "            image_path = join(driver_dataset_folder_path, 'train', 'c%s'%i)\n",
    "            if os.path.exists(join(image_path, row['img'])):\n",
    "                if not os.path.exists(join(train5_dir, 'c%s'%i)):\n",
    "                    os.makedirs(join(train5_dir, 'c%s'%i))\n",
    "                shutil.copy(join(image_path, row['img']), join(train5_dir, 'c%s'%i,row['img']))\n",
    "                #os.symlink(join(image_path, row['img']), join(train1_dir, 'c%s'%i,row['img']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18047 images belonging to 10 classes.\n",
      "Found 4377 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "xception_train_generator5 = xception_train_datagen.flow_from_directory(\n",
    "    train5_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical')\n",
    "\n",
    "xception_valid_generator5 = xception_valid_datagen.flow_from_directory(\n",
    "    valid5_dir,\n",
    "    target_size=(299, 299),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5 = Model(inputs=input, outputs=x) \n",
    "\n",
    "model_5.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1128/1128 [==============================] - 597s 529ms/step - loss: 0.0278 - categorical_accuracy: 0.9926 - val_loss: 0.6280 - val_categorical_accuracy: 0.8211\n",
      "Epoch 2/2\n",
      "1128/1128 [==============================] - 597s 529ms/step - loss: 0.0210 - categorical_accuracy: 0.9947 - val_loss: 0.6258 - val_categorical_accuracy: 0.8271\n"
     ]
    }
   ],
   "source": [
    "history_5 = model_5.fit_generator(\n",
    "    xception_train_generator5,\n",
    "    epochs=2,,\n",
    "    validation_data=xception_valid_generator5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xception model saved.\n"
     ]
    }
   ],
   "source": [
    "model_5.save(join(pardir, 'model', 'xception_5.h5'))\n",
    "print(\"xception model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "model_xception_5 = load_model(join(pardir, 'model', 'xception_5.h5'))\n",
    "print(\"Model loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2492/2492 [==============================] - 1278s 513ms/step\n",
      "(79726, 10)\n"
     ]
    }
   ],
   "source": [
    "pred_xception_5 = model_xception_5.predict_generator(xception_test_generator, verbose=1)\n",
    "print(pred_xception_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_xception_5 = np.append(image_names, pred_xception_5, axis = 1)\n",
    "predict_result_xception_5 = pd.DataFrame(result_xception_5, \n",
    "            columns=['img', 'c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n",
    "predict_result_xception_5.to_csv('predict_result_xception_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 融合5个模型的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = pd.read_csv('sample_submission.csv')['img']\n",
    "category = ['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1 = pd.read_csv('predict_result_xception_1.csv')\n",
    "result2 = pd.read_csv('predict_result_xception_2.csv')\n",
    "result3 = pd.read_csv('predict_result_xception_3.csv')\n",
    "result4 = pd.read_csv('predict_result_xception_4.csv')\n",
    "result5 = pd.read_csv('predict_result_xception_5.csv')\n",
    "\n",
    "average = (result1[category] + result2[category] + result3[category] + result4[category] + result5[category])/5\n",
    "\n",
    "average.insert(0, 'img', pd.Series(img_name))\n",
    "\n",
    "average.to_csv('Xception_35_bagging.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 融合3个模型的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = pd.read_csv('predict_result_xception_2.csv')\n",
    "result3 = pd.read_csv('predict_result_xception_3.csv')\n",
    "result4 = pd.read_csv('predict_result_xception_4.csv')\n",
    "\n",
    "average = (result2[category] + result3[category] + result4[category])/3\n",
    "\n",
    "average.insert(0, 'img', pd.Series(img_name))\n",
    "\n",
    "average.to_csv('Xception_35_bagging_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
